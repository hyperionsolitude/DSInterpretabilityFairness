{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIE5znofC4is"
      },
      "source": [
        "**DS530.V: Fairness and Interpretability in Data Science**\n",
        "\n",
        "Class Project:  Fairness Analysis via Different ML Models on the UCI's Adult Dataset\n",
        "\n",
        "*Author:* Utku Acar  \n",
        "*Department:* Computer Science  \n",
        "*Role:* Software Engineer / Researcher  \n",
        "\n",
        "*My Secret Power:* I've got a knack for efficiently harnessing prompts and taming the wildest multi-dimensional data, especially in the realm of Images and Videos, through the magic of Deep Learning and Computer Vision.\n",
        "\n",
        "\n",
        "*My Crime-Fighting Identity:* Get ready to meet the one and only Hyperion Solitude, because I'm about to dive into a world of adventure where data science meets supercharged strategies! 🚀🔍📊🦸‍♂️      \n",
        "\n",
        "*Stay tuned for the thrilling odyssey, unfolding soon at https://github.com/hyperionsolitude. The adventure of a lifetime awaits! 🌟🎉*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting the dataset \"Adult\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W35wC6NDATn",
        "outputId": "c859b928-5f3b-43fe-9392-bef57fabcf8c"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Discuss why you need to treat fairness issues in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjsB68TEC4iu",
        "outputId": "4d695cbd-afa7-4ea6-82e1-f819b88d0d46"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "adult = fetch_ucirepo(id=2)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = adult.data.features\n",
        "y = adult.data.targets\n",
        "\n",
        "# metadata\n",
        "print(adult.metadata)\n",
        "\n",
        "# variable information\n",
        "print(adult.variables)\n",
        "\n",
        "# Accessing and printing the description from the metadata\n",
        "for name, description in zip(adult.variables['name'], adult.variables['description']):\n",
        "    print(name, \":\", description)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmDJOp8RCMBR"
      },
      "source": [
        "## Task 2.1: Find a good black-box classification model to predict the target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9zZ8c_iC4iv",
        "outputId": "860bbd3a-be2f-4eae-c372-874e9dbc96b9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Concatenate X and y for easier handling\n",
        "data = pd.concat([X, y], axis=1)\n",
        "\n",
        "data = data.replace('?', np.nan)\n",
        "\n",
        "# Find rows with missing values and drop them\n",
        "data = data.dropna()\n",
        "\n",
        "binary_cols = ['sex','income']  # Add other binary column names if present\n",
        "categorical_cols = [col for col in data.columns if col not in binary_cols and data[col].dtype == 'object']\n",
        "\n",
        "# Separate binary and categorical columns\n",
        "binary_data = data[binary_cols]\n",
        "categorical_data = data[categorical_cols]\n",
        "\n",
        "# Process categorical features into dummies\n",
        "categorical_data = pd.get_dummies(categorical_data)\n",
        "\n",
        "# Combine processed categorical features and unchanged binary features\n",
        "processed_data = pd.concat([categorical_data, binary_data], axis=1)\n",
        "\n",
        "# Separate X and y after processing\n",
        "X = processed_data.drop('income', axis=1)\n",
        "y = processed_data['income']\n",
        "\n",
        "# Convert target variable to float32\n",
        "y = (y == '>50K').astype('float32')\n",
        "\n",
        "X['sex'] = X['sex'].map({'Female': 0, 'Male': 1})\n",
        "X = X.astype('float32')\n",
        "\n",
        "# Split the data into training, validation, and test sets (once)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Define ModelCheckpoint to save the best model\n",
        "checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True, mode='max', verbose=0)\n",
        "\n",
        "# Initialize a list to store the test losses and accuracies\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "best_accuracy = 0.0  # Track the best accuracy\n",
        "best_loss = float('inf')  # Track the best loss\n",
        "\n",
        "# Run the model 10 times\n",
        "for i in range(10):\n",
        "    # Define the model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
        "    model.add(Dense(64, activation='relu'))  # Hidden layer\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping, checkpoint], verbose=0)\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Run {i+1}:')\n",
        "    print('Test loss:', score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "    # Append the test accuracy to the list\n",
        "    test_losses.append(score[0])\n",
        "    test_accuracies.append(score[1])\n",
        "\n",
        "    # Check if the current accuracy is better than the previous best\n",
        "    if score[1] > best_accuracy:\n",
        "        print(f\"New best accuracy found! Saving the model.\")\n",
        "        best_accuracy = score[1]\n",
        "        model.save_weights('best_weights.keras')\n",
        "\n",
        "    if score[0] < best_loss:\n",
        "        best_loss = score[0]\n",
        "\n",
        "    # Check for early stopping based on accuracy and loss improvement\n",
        "    if i > 0 and (abs(test_accuracies[i] - best_accuracy) < 0.001 or abs(test_losses[i] - best_loss) < 0.001):\n",
        "        print(f\"No significant improvement from previous run. Stopping at iteration {i+1}.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh4RyK6NCmqx"
      },
      "source": [
        "## Task 2.2:  Calculate KPIs regarding fairness based on predictions of this black box model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5W-t9d0cdxi",
        "outputId": "ee7542d3-ce26-4f50-f75a-e349c2aea274"
      },
      "outputs": [],
      "source": [
        "!pip install aif360\n",
        "!pip install fairlearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJLlL2trcVDi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from fairlearn.metrics import equalized_odds_difference, demographic_parity_difference, equalized_odds_ratio\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calculate_fairness_metrics(X_test, y_test, y_pred):\n",
        "    y_pred_binary = (y_pred > 0.5).astype('int')\n",
        "\n",
        "    eod_results = []\n",
        "    insufficient_features = 0\n",
        "\n",
        "    for feature in X_test.columns:\n",
        "        group_0 = (X_test[feature] == 0)\n",
        "        group_1 = (X_test[feature] == 1)\n",
        "\n",
        "        if len(np.unique(y_test[group_0])) < 2 or len(np.unique(y_test[group_1])) < 2:\n",
        "            insufficient_features += 1\n",
        "            continue\n",
        "\n",
        "        conf_matrix_0 = confusion_matrix(y_test[group_0], y_pred_binary[group_0])\n",
        "        conf_matrix_1 = confusion_matrix(y_test[group_1], y_pred_binary[group_1])\n",
        "\n",
        "        tn0, fp0, fn0, tp0 = conf_matrix_0.ravel()\n",
        "        tn1, fp1, fn1, tp1 = conf_matrix_1.ravel()\n",
        "\n",
        "        tpr0 = tp0 / (tp0 + fn0)\n",
        "        fpr0 = fp0 / (fp0 + tn0)\n",
        "        tpr1 = tp1 / (tp1 + fn1)\n",
        "        fpr1 = fp1 / (fp1 + tn1)\n",
        "\n",
        "        eod_value = equalized_odds_difference(y_test, y_pred_binary, sensitive_features=X_test[feature])\n",
        "        pp_value = demographic_parity_difference(y_test, y_pred_binary, sensitive_features=X_test[feature])\n",
        "        sp_value = equalized_odds_ratio(y_test, y_pred_binary, sensitive_features=X_test[feature])\n",
        "        #mean_diff_nn = np.abs(np.mean(y_pred_binary[X_test[feature] == 1]) - np.mean(y_pred_binary[X_test[feature] == 0])) # Same with PP\n",
        "        fpr_sensitive = np.mean(y_pred_binary[(y_test == 0) & (X_test[feature] == 1)])\n",
        "        fpr_non_sensitive = np.mean(y_pred_binary[(y_test == 0) & (X_test[feature] == 0)])\n",
        "        disparate_impact_nn = fpr_sensitive / fpr_non_sensitive\n",
        "\n",
        "        eod_results.append((feature, eod_value, pp_value, sp_value, disparate_impact_nn, tpr0, fpr0, tpr1, fpr1))\n",
        "\n",
        "    print(\"There are\", insufficient_features, \"features with insufficient data.\")\n",
        "\n",
        "    sorted_eod_results = sorted(eod_results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "    pd.set_option('display.precision', 3)\n",
        "\n",
        "    kpi_df = pd.DataFrame(sorted_eod_results, columns=['Feature Name', 'EOD Value', 'Predictive Parity', 'Statistical Parity', 'Disparate Impact', 'TPR for 0', 'FPR for 0', 'TPR for 1', 'FPR for 1'])\n",
        "\n",
        "    print(\"\\n\\n\", kpi_df.to_string(index=False))\n",
        "\n",
        "    return kpi_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDWXcN-XFBO6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_table_image(dataframe, filename):\n",
        "    formatted_df = dataframe.round(3)\n",
        "    csv_filename = filename.replace('.png', '_data.csv')  # Adjust CSV filename\n",
        "    formatted_df.to_csv(csv_filename, index=True)  # Save DataFrame to CSV file\n",
        "    # Create the table figure\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.table(cellText=formatted_df.values,\n",
        "              colLabels=formatted_df.columns,\n",
        "              rowLabels=formatted_df.index,\n",
        "              cellLoc='center',\n",
        "              loc='center')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Save the table figure with the title below\n",
        "    plt.savefig(filename, bbox_inches='tight', dpi=800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yHwbR-SY_-d0",
        "outputId": "4d50ca08-c6bd-4694-80d0-4e4a8626b0dc"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "kpi_nn_df = calculate_fairness_metrics(X_test, y_test,y_pred)\n",
        "create_table_image(kpi_nn_df, 'kpi_metrics_table_nn.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89jGZv4qCvJ8"
      },
      "source": [
        "## Task 3.1: Use a global surrogate model for a model agnostic explanation. (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S04K8f1fqJqw",
        "outputId": "20010fd9-361f-4709-901c-c8423a672661"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create and train a logistic regression model on the predictions of the neural network\n",
        "surrogate_model_LR = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Obtain predictions of the neural network on the validation set\n",
        "y_val_pred_nn = model.predict(X_train_val)\n",
        "\n",
        "# Train the logistic regression model on the neural network predictions\n",
        "surrogate_model_LR.fit(y_val_pred_nn, y_train_val)\n",
        "\n",
        "# Get predictions of the neural network on the test set\n",
        "y_test_pred_nn = model.predict(X_test)\n",
        "\n",
        "# Use the trained logistic regression as a surrogate to predict on the neural network outputs\n",
        "y_test_pred_surrogate_LR = surrogate_model_LR.predict(y_test_pred_nn)\n",
        "\n",
        "# Evaluate the surrogate model's performance\n",
        "surrogate_accuracy_LR = surrogate_model_LR.score(y_test_pred_nn, y_test)\n",
        "print(f\"Surrogate Model(Log-Reg) Accuracy: {surrogate_accuracy_LR}\")\n",
        "\n",
        "# Now, interpret the coefficients of the logistic regression model to understand feature importance\n",
        "surrogate_coefficients_LR = surrogate_model_LR.coef_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8b6ALlhDC81"
      },
      "source": [
        "## Task 3.2: Use a global surrogate model for a model agnostic explanation. (Decision Tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "2l3uWJDHsZ8r",
        "outputId": "44812dd3-92b1-4247-9d19-e34e79ef09f5"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Get predictions of the neural network on the validation set\n",
        "y_val_pred_nn = model.predict(X_train_val)\n",
        "\n",
        "# Create a Decision Tree classifier as a surrogate model\n",
        "surrogate_model_DT = DecisionTreeClassifier(max_depth=10)  # Adjust max_depth as needed\n",
        "\n",
        "# Train the surrogate model on the neural network predictions and the original labels\n",
        "surrogate_model_DT.fit(y_val_pred_nn, y_train_val)\n",
        "\n",
        "# Get predictions of the neural network on the test set\n",
        "y_test_pred_nn = model.predict(X_test)\n",
        "\n",
        "# Get predictions of the surrogate model on the neural network predictions\n",
        "y_test_pred_surrogate_DT = surrogate_model_DT.predict(y_test_pred_nn)\n",
        "\n",
        "# Evaluate the surrogate model's performance (accuracy in this case)\n",
        "surrogate_accuracy_DT = accuracy_score(y_test, y_test_pred_surrogate_DT)\n",
        "print(f\"Surrogate Model(Dec-Tre) Accuracy: {surrogate_accuracy_DT}\")\n",
        "\n",
        "# Optional: Visualize the surrogate tree (if depth is manageable)\n",
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6),dpi=800)\n",
        "plot_tree(surrogate_model_DT, filled=True, feature_names=X_train_val.columns)\n",
        "plt.savefig('surrogate_model_DT.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYjeE1SIHWcb"
      },
      "source": [
        "## Task 4.1: Discuss if the fairness issues have increased or decreased in the global surrogate model using the KPIs introduced in the class.(Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W6lRg3NDl6_"
      },
      "outputs": [],
      "source": [
        "def calculate_average_kpis(kpi_dataframe,modelname):\n",
        "    # Replace 'inf' values with NaN\n",
        "    kpi_dataframe.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    avg_kpis = kpi_dataframe.iloc[:, 1:].mean()\n",
        "    print(print(\"Average KPI Values for\",modelname,\"Model\"))\n",
        "    return avg_kpis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slKvYVz6N7Al"
      },
      "outputs": [],
      "source": [
        "def compare_kpi_features(model_names, *kpi_dataframes):\n",
        "    # Transpose each dataframe\n",
        "    transposed_dfs = [df.T for df in kpi_dataframes]\n",
        "\n",
        "    # Concatenate the transposed dataframes along rows\n",
        "    comparison_df = pd.concat(transposed_dfs, axis=1)\n",
        "\n",
        "    # Set columns with model names\n",
        "    comparison_df.columns = model_names\n",
        "\n",
        "    # Calculate differences and percentage differences between models\n",
        "    for idx, model_name in enumerate(model_names[1:], start=2):\n",
        "        diff_col = f\"Difference_{model_name}\"\n",
        "        perc_diff_col = f\"Percentage_Difference_{model_name}\"\n",
        "        comparison_df[diff_col] = comparison_df[model_name] - comparison_df[model_names[0]]\n",
        "        comparison_df[perc_diff_col] = (comparison_df[diff_col] / comparison_df[model_names[0]]) * 100\n",
        "\n",
        "    return comparison_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ccj0nTX9Aqzv",
        "outputId": "739850a4-d90c-4bb5-9808-5a82c0f7cb32"
      },
      "outputs": [],
      "source": [
        "kpi_lr_df = calculate_fairness_metrics(X_test, y_test,y_test_pred_surrogate_LR)\n",
        "create_table_image(kpi_lr_df, 'kpi_metrics_table_lr.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1l_WkmQL7P6a",
        "outputId": "24fef448-7dbb-4d10-b62f-9796816774dd"
      },
      "outputs": [],
      "source": [
        "average_kpis_nn = calculate_average_kpis(kpi_nn_df,\"Neural Network\")\n",
        "print(average_kpis_nn)\n",
        "\n",
        "average_kpis_lr_sur = calculate_average_kpis(kpi_lr_df,\"Logistic Regression Surrogate\")\n",
        "print(average_kpis_lr_sur)\n",
        "\n",
        "comparison_nn_lr_sur = compare_kpi_features([\"NN\", \"LR Sur\"],average_kpis_nn, average_kpis_lr_sur)\n",
        "print(comparison_nn_lr_sur)\n",
        "\n",
        "create_table_image(comparison_nn_lr_sur, 'comparison_nn_lr_sur.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubKlMAoXCPh6"
      },
      "source": [
        "## Task 4.2: Discuss if the fairness issues have increased or decreased in the global surrogate model using the KPIs introduced in the class.(Decision Tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8EeUToeVCWgZ",
        "outputId": "3ad99b73-fb5f-4a30-8272-f7d76b4f7797"
      },
      "outputs": [],
      "source": [
        "kpi_dt_df = calculate_fairness_metrics(X_test, y_test,y_test_pred_surrogate_DT)\n",
        "create_table_image(kpi_dt_df, 'kpi_metrics_table_dt.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "14rfR_pO2ZE7",
        "outputId": "a1a26776-0ba6-47f8-fa13-d66100ea9ecb"
      },
      "outputs": [],
      "source": [
        "average_kpis_nn = calculate_average_kpis(kpi_nn_df,\"Neural Network\")\n",
        "print(average_kpis_nn)\n",
        "\n",
        "average_kpis_dt_sur = calculate_average_kpis(kpi_dt_df,\"Decision Tree Surrogate\")\n",
        "print(average_kpis_dt_sur)\n",
        "\n",
        "comparison_nn_dt_sur = compare_kpi_features([\"NN\", \"DT Sur\"],average_kpis_nn, average_kpis_dt_sur)\n",
        "print(comparison_nn_dt_sur)\n",
        "\n",
        "create_table_image(comparison_nn_dt_sur, 'comparison_nn_dt_sur.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk84irjQHJIq"
      },
      "source": [
        "## Task 5.1: Find a good interpretable classification model to predict the target variable. (Decision Tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "NBmxKiQXzGPA",
        "outputId": "1c02136f-fbee-4966-cd41-637ee7bc1b04"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "# Train a Decision Tree classifier\n",
        "interpretable_model_DT = DecisionTreeClassifier(max_depth=10)  # Adjust max_depth as needed\n",
        "interpretable_model_DT.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions for test set\n",
        "y_pred_interpretable_DT = interpretable_model_DT.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_interpretable_DT = accuracy_score(y_test, y_pred_interpretable_DT)\n",
        "\n",
        "plt.figure(figsize=(12, 6),dpi=800)\n",
        "plot_tree(interpretable_model_DT, filled=True, feature_names=X_train.columns)\n",
        "plt.savefig('interpretable_model_DT.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFOMLd8os7gG"
      },
      "source": [
        "## Task 5.1: Find a good interpretable classification model to predict the target variable. (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ImQ7uBxsCTm"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "# Train a Logistic Regression classifier\n",
        "interpretable_model_LR = LogisticRegression(max_iter=1000)\n",
        "interpretable_model_LR.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions for test set\n",
        "y_pred_interpretable_LR = interpretable_model_LR.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "\n",
        "accuracy_interpretable_LR = accuracy_score(y_test, y_pred_interpretable_LR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp9P1XaqG-tA"
      },
      "source": [
        "## Task 5.2.1: Compare the results of surrogate model and interpretable model in terms of model accuracy and fairness. (Logical Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1J-OYbT7zZmH",
        "outputId": "7f530cd1-1d82-410f-b593-4bdde1e18e3b"
      },
      "outputs": [],
      "source": [
        "kpi_lrmod_df = calculate_fairness_metrics(X_test, y_test,y_pred_interpretable_LR)\n",
        "create_table_image(kpi_lrmod_df, 'kpi_metrics_table_lrmod.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VTCOa7gQD5e7",
        "outputId": "bbbdb8f8-a5fb-4045-84f7-c33a3779d1c5"
      },
      "outputs": [],
      "source": [
        "average_kpis_lr = calculate_average_kpis(kpi_lrmod_df,\"Interpretable Logistic Regression\")\n",
        "print(average_kpis_lr)\n",
        "\n",
        "\n",
        "comparison_surr_lr_int = compare_kpi_features([\"LR Sur\", \"LR\"],average_kpis_lr_sur, average_kpis_lr)\n",
        "accuracy_row_LR = pd.Series({\n",
        "    \"LR Sur\": surrogate_accuracy_LR,\n",
        "    \"LR\": accuracy_interpretable_LR,\n",
        "    \"Difference_LR\":  accuracy_interpretable_LR - surrogate_accuracy_LR,\n",
        "    \"Percentage_Difference_LR\": 100 * (accuracy_interpretable_LR - surrogate_accuracy_LR) / accuracy_interpretable_LR\n",
        "})\n",
        "comparison_surr_lr_int = pd.concat([accuracy_row_LR.to_frame().T, comparison_surr_lr_int])\n",
        "comparison_surr_lr_int = comparison_surr_lr_int.rename(index={comparison_surr_lr_int.index[0]: 'Accuracy'})\n",
        "print(comparison_surr_lr_int)\n",
        "\n",
        "create_table_image(comparison_surr_lr_int, 'comparison_surr_lr_int.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNI83C62Pgzq"
      },
      "source": [
        "## Task 5.2.2: Compare the results of surrogate model and interpretable model in terms of model accuracy and fairness.(Decision Tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RwwlCeZrOXJb",
        "outputId": "1fb14bea-c33d-4606-d860-e63b263f409b"
      },
      "outputs": [],
      "source": [
        "kpi_dtmod_df = calculate_fairness_metrics(X_test, y_test,y_pred_interpretable_DT)\n",
        "create_table_image(kpi_dtmod_df, 'kpi_metrics_table_dt.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ijdyI7afEGgB",
        "outputId": "f37fa852-287f-475f-806f-6b02e399e30b"
      },
      "outputs": [],
      "source": [
        "average_kpis_dt = calculate_average_kpis(kpi_dtmod_df,\"Interpretable Decision Tree\")\n",
        "print(average_kpis_dt)\n",
        "\n",
        "comparison_surr_dt_int = compare_kpi_features([\"DT Sur\", \"DT\"],average_kpis_dt_sur,average_kpis_dt)\n",
        "accuracy_row_DT = pd.Series({\n",
        "    \"DT Sur\": surrogate_accuracy_DT,\n",
        "    \"DT\": accuracy_interpretable_DT,\n",
        "    \"Difference_DT\": accuracy_interpretable_DT - surrogate_accuracy_DT,\n",
        "    \"Percentage_Difference_DT\": 100 * (accuracy_interpretable_DT - surrogate_accuracy_DT) / accuracy_interpretable_DT\n",
        "})\n",
        "\n",
        "comparison_surr_dt_int = pd.concat([accuracy_row_DT.to_frame().T, comparison_surr_dt_int])\n",
        "comparison_surr_dt_int = comparison_surr_dt_int.rename(index={comparison_surr_dt_int.index[0]: 'Accuracy'})\n",
        "print(comparison_surr_dt_int)\n",
        "\n",
        "create_table_image(comparison_surr_dt_int, 'comparison_surr_dt_int.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2a83XPzoH3o"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_table_image_sum(dataframe, filename):\n",
        "    # Select the first and last 5 rows of the dataframe\n",
        "    top_and_bottom = pd.concat([dataframe.head(5), dataframe.tail(5)])\n",
        "    formatted_df = top_and_bottom.round(3)\n",
        "\n",
        "    csv_filename = filename.replace('.png', '_data.csv')  # Adjust CSV filename\n",
        "    formatted_df.to_csv(csv_filename, index=True)  # Save DataFrame to CSV file\n",
        "\n",
        "    # Create the table figure with larger font size\n",
        "    plt.figure(figsize=(12, 4))  # Adjust the height to make it more compact\n",
        "    table = plt.table(cellText=formatted_df.values,\n",
        "                      colLabels=formatted_df.columns,\n",
        "                      rowLabels=formatted_df.index,\n",
        "                      cellLoc='center',\n",
        "                      loc='center')\n",
        "    table.auto_set_font_size(True)\n",
        "    # table.set_fontsize(12)  # Set your desired font size here\n",
        "    table.scale(1.2, 1.2)   # Adjust the table size if needed\n",
        "    plt.axis('off')\n",
        "    # Adjust subplot parameters to remove excess space\n",
        "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
        "    # Save the table figure with the title below\n",
        "    plt.savefig(filename, bbox_inches='tight', dpi=800)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "qWxSijnHp2Gg",
        "outputId": "4f68f2fa-2362-4e52-fdd5-cbc1925c93d2"
      },
      "outputs": [],
      "source": [
        "create_table_image_sum(kpi_nn_df, 'kpi_metrics_table_nn_sum.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "9tlTmR7Sp4Kx",
        "outputId": "8549bdd4-9817-47cf-ae20-0b89d5abb250"
      },
      "outputs": [],
      "source": [
        "create_table_image_sum(kpi_lr_df, 'kpi_metrics_table_lr_sum.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "C-QdZbvYp8q0",
        "outputId": "f96a42b0-011e-417e-b312-ce295cabd8ae"
      },
      "outputs": [],
      "source": [
        "create_table_image_sum(kpi_dt_df, 'kpi_metrics_table_dt_sum.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "J5LHai2pqAx6",
        "outputId": "14d7bc9d-46be-4ade-fbd3-a67b175c8b45"
      },
      "outputs": [],
      "source": [
        "create_table_image_sum(kpi_lrmod_df, 'kpi_metrics_table_lrmod_sum.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "sfnsfOdhqCXD",
        "outputId": "26e55c59-ad0e-41bb-dfaf-0894b9fe14d4"
      },
      "outputs": [],
      "source": [
        "create_table_image_sum(kpi_dtmod_df, 'kpi_metrics_table_dtmod_sum.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extra code snippet for transforming csv's to Latex formatted tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Directory containing CSV files\n",
        "directory = './'  # Replace './' with your directory containing CSV files\n",
        "\n",
        "# Output file name\n",
        "output_filename = 'output_table.tex'\n",
        "\n",
        "# Function to convert CSV to LaTeX table\n",
        "def csv_to_latex(csv_file):\n",
        "    data = pd.read_csv(csv_file)\n",
        "    data.columns = [header.replace('_', ' ') for header in data.columns]\n",
        "\n",
        "    latex_table = \"\\\\begin{table}[H]\\n\"\n",
        "    latex_table += \"  \\\\centering\\n\"\n",
        "    latex_table += \"  \\\\begin{tabular*}{\\\\textwidth}{@{\\\\extracolsep{\\\\fill}} \" + \"l\" * len(data.columns) + \"}\\\\toprule\\n\"\n",
        "    latex_table += \"    \" + \" & \".join(data.columns) + \" \\\\\\\\ \\\\midrule\\n\"\n",
        "    \n",
        "    for _, row in data.iterrows():\n",
        "        latex_table += \"    \" + \" & \".join([str(val).replace('_', '\\\\_') for val in row.tolist()]) + \" \\\\\\\\ \\n\"\n",
        "\n",
        "    latex_table += \"    \\\\bottomrule\\n\"\n",
        "    latex_table += \"  \\\\end{tabular*}\\n\"\n",
        "    latex_table += f\"  \\\\caption{{Comparison of KPIs: {os.path.splitext(os.path.basename(csv_file))[0].replace('_', ' ')}}}\\n\"\n",
        "    latex_table += f\"  \\\\label{{tab:{os.path.splitext(os.path.basename(csv_file))[0]}}}\\n\"\n",
        "    latex_table += \"\\\\end{table}\\n\\n\"\n",
        "\n",
        "    return latex_table\n",
        "\n",
        "# Loop through CSV files in the directory and generate LaTeX tables\n",
        "latex_output = \"\"\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        latex_output += csv_to_latex(file_path)\n",
        "\n",
        "# Write the LaTeX tables to a single file\n",
        "with open(output_filename, 'w') as file:\n",
        "    file.write(latex_output)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
